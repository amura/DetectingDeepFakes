{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DetectingDeepFakesPart1_faceprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amura/DetectingDeepFakes/blob/faceprocessing/DetectingDeepFakesPart1_faceprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oCEsLH4hICw"
      },
      "source": [
        "# Exploring dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qho01dsilsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a248ca-c544-46e2-b93a-6f950ea99b88"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLbrVbbmiqBf"
      },
      "source": [
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import pprint\n",
        "import json\n",
        "from PIL import Image\n",
        "from PIL.ImageStat import Stat\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import cv2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEJZHYRsrN8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851a5204-8947-45a8-fbb1-8b2dd6103bab"
      },
      "source": [
        "# mmove images to folder for use later\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryZFQLEirLzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f946cf-095b-42da-c963-d70007260967"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/deepfakes\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/deepfakes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU-Xv6n82OZZ"
      },
      "source": [
        "### face detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMQrKvfyg7Vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def27e02-80ae-4173-d28f-b812b07f5d29"
      },
      "source": [
        "# install MTCNN\n",
        "!pip install mtcnn \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mtcnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (3.1.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.5.2)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJe0hUZZX2EP"
      },
      "source": [
        "Function for extracting the coordinates for the top left bounding box, width and height, and location of eyes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uISL53MkQQG"
      },
      "source": [
        "from mtcnn import MTCNN\n",
        "import cv2\n",
        "\n",
        "class DeepFakesDetector:\n",
        "   # desired left eye location:  common to see percentages within the range of 20-40%. This controls the amount of face visible in final image\n",
        "   def __init__(self,  desired_size=256, desired_lefteye_location=(0.35,0.35)):\n",
        "      self.desired_size = desired_size\n",
        "      self.desired_lefteye_location = desired_lefteye_location\n",
        "\n",
        "  # this extracts the bounding box as well as the eye location\n",
        "   @staticmethod\n",
        "   def extractFaceFromImage( filePath, faceIndex = 0):\n",
        "      #detect face\n",
        "      img = cv2.cvtColor(cv2.imread(filePath), cv2.COLOR_BGR2RGB)\n",
        "      detector = MTCNN()\n",
        "      result = detector.detect_faces(img)\n",
        "\n",
        "      # assumes only single person in the picture so index is 0\n",
        "      return  result[faceIndex]['box'],  result[faceIndex]['keypoints']['left_eye'],  result[faceIndex]['keypoints']['right_eye'], img\n",
        "       \n",
        "   def crop_and_align(self, img, left, right):\n",
        "      angle, dX, dY = self.eyes_angle(left, right)\n",
        "      \n",
        "      #work out scaling factor\n",
        "      self.scaling_factor(dX, dY)\n",
        "\n",
        "      mat = self.rotation_matrix(angle, left, right)\n",
        "\n",
        "      return self.transform(img, mat)\n",
        "\n",
        "  # This gets the angle between the eyes\n",
        "   def eyes_angle(self,left, right):\n",
        "      dY = right[1] - left[1]\n",
        "      dX = right[0] - left[0]\n",
        "      return np.degrees(np.arctan2(dY, dX)), dX, dY\n",
        "\n",
        "\n",
        "   def scaling_factor(self, dX, dY):\n",
        "      desiredRightEyeX = 1.0 - self.desired_lefteye_location[0]\n",
        "      hyp = np.sqrt((dX ** 2) + (dY ** 2))\n",
        "      desiredDist = (desiredRightEyeX - self.desired_lefteye_location[0])\n",
        "      desiredDist *= self.desired_size\n",
        "      self.scale = desiredDist / hyp\n",
        "\n",
        "   def rotation_matrix(self, angle, left, right):\n",
        "      # compute center (x, y)-coordinates (i.e., the median point)\n",
        "      # between the two eyes in the input image\n",
        "      eyesCenter = ((left[0] + right[0]) // 2,  (left[1] + right[1]) // 2)\n",
        "\n",
        "      # grab the rotation matrix for rotating and scaling the face\n",
        "      M = cv2.getRotationMatrix2D(eyesCenter, angle, self.scale)\n",
        "         \n",
        "      # update the translation component of the matrix\n",
        "      tX = self.desired_size * 0.5\n",
        "      tY = self.desired_size * self.desired_lefteye_location[1]\n",
        "      M[0, 2] += (tX - eyesCenter[0])\n",
        "      M[1, 2] += (tY - eyesCenter[1])     \n",
        "\n",
        "      return M\n",
        "\n",
        "   def transform(self, image, M):\n",
        "      # apply the affine transformation\n",
        "      #(w,h) = (self.desired_size, self.desired_size)\n",
        "\t\t  return cv2.warpAffine(image, M, (self.desired_size, self.desired_size),flags=cv2.INTER_CUBIC)\n",
        "      \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr6YrdA0fpJj"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "bb, left, right, img = DeepFakesDetector.extractFaceFromImage('./deepfake_sa1_output/00001.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRFBNmRUlDa9"
      },
      "source": [
        "det = DeepFakesDetector()\n",
        "alignedFace = det.crop_and_align(img,left, right)\n",
        "cv2_imshow(cv2.cvtColor(alignedFace, cv2.COLOR_RGB2BGR))\n",
        "cv2_imshow(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-yGg5kErMi0"
      },
      "source": [
        "# !rm -rf ./deepfake_sa1_processed/\n",
        "# !rm -rf ./real_sa1_processed/"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "019U5jTUmFkV"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "##setup output folders\n",
        "deepFakeProcessed = './deepfake_sa1_processed'\n",
        "realProcessed = './real_sa1_processed'\n",
        "try:\n",
        "      os.makedirs(deepFakeProcessed)\n",
        "except OSError:\n",
        "      pass\n",
        "\n",
        "try:\n",
        "      os.makedirs(realProcessed)\n",
        "except OSError:\n",
        "      pass\n",
        "\n",
        "try:\n",
        "      #make sure folders are clean\n",
        "      !rm -rf ./deepfake_sa1_processed/\n",
        "      !rm -rf ./real_sa1_processed/      \n",
        "except OSError:\n",
        "      pass\n",
        "\n",
        "det = DeepFakesDetector()\n",
        "# process all faces in directory and save as png\n",
        "for imgPath in glob.glob('./deepfake_sa1_output/*.jpg'):\n",
        "    baseFileName = os.path.basename(imgPath)\n",
        "    outputFileName = os.path.join(deepFakeProcessed,os.path.splitext(baseFileName)[0] + '.png')      \n",
        "    bb, left, right, img = DeepFakesDetector.extractFaceFromImage(imgPath)  \n",
        "    alignedFace = det.crop_and_align(img,left, right)      \n",
        "    cv2.imwrite(outputFileName, cv2.cvtColor(alignedFace, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "for imgPath in glob.glob('./real_sa1_output/*.jpg'):\n",
        "    baseFileName = os.path.basename(imgPath)\n",
        "    outputFileName = os.path.join(realProcessed,os.path.splitext(baseFileName)[0] + '.png')      \n",
        "    bb, left, right, img = DeepFakesDetector.extractFaceFromImage(imgPath)  \n",
        "    alignedFace = det.crop_and_align(img,left, right)      \n",
        "    cv2.imwrite(outputFileName, cv2.cvtColor(alignedFace, cv2.COLOR_RGB2BGR))    \n",
        "  "
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}